# ğŸ  Linear Regression & Feature Engineering Analysis

## ğŸ“– Introduction

This project demonstrates **linear regression** on a dataset with multiple features.
The focus is on **feature engineering**, applying **transformations** to stabilize variance, and evaluating the model using various metrics.

---

## ğŸ“‚ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ datasets/                # Raw and processed datasets
â”‚   â”œâ”€â”€ Housing.csv
â”‚   â””â”€â”€ Housing-Clean.csv
â”‚
â”œâ”€â”€ models/                  # Saved models
â”‚   â””â”€â”€ model.pkl
â”‚
â”œâ”€â”€ plots/                   # Visualizations
â”‚   â”œâ”€â”€ pred_vs_actual.png
â”‚   â”œâ”€â”€ residuals.png
â”‚   â””â”€â”€ r2_vs_transforms.png
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ regression-explorer.ipynb
â””â”€â”€ README.md
```

---

## ğŸ§¹ Data Preprocessing

* Features split into **numeric** and **binary** types.
* New features created to improve model performance:

  * `total_rooms`
  * `area_per_bedroom`
  * `total_features`
* **Standardization** applied to numeric features for better regression performance.

---

## ğŸ“ˆ Single Feature Regression

* Initial regression using **only the `area` feature**.
* Limited predictive power: **RÂ² â‰ˆ 0.2729**.

---

## ğŸ“Š Multiple Feature Regression

* Regression using **all features**.
* Significant performance improvement: **RÂ² â‰ˆ 0.6529**.
* **Feature engineering** further enhanced model performance.

---

## ğŸ”„ Transformations

* Tested transformations on the **target variable**:

  * Cube-root
  * Log
  * Yeo-Johnson
* **RÂ² scores** compared across transformations.
* **Cube-root transformation** provided the highest RÂ² score.

---

## ğŸ›  Regularization

* Methods tested: **Ridge, Lasso, ElasticNet**.
* Ridge regression provided slightly **better coefficient stability**, but minimal RÂ² improvement.

---

## ğŸ† Results

* Cube-root transformation + feature engineering = **best results**.
* Ridge regularization can be optionally applied for **more stable coefficients**.

---

## ğŸ”š Conclusion

* Linear regression assumes **linear relationships**, limiting predictive power for this dataset.
* **Feature engineering** and **transformations** improve performance but **perfect prediction is not achievable** with this approach.
* The **final model** is saved and ready for future predictions.

---

## ğŸ’¾ Model Storage

* Trained model saved using `joblib`:

```python
import joblib
joblib.dump(model, 'models/model.pkl')
```

---

## âš¡ Dependencies

Install required packages using:

```bash
pip install -r requirements.txt
```

---

âœ¨ **Happy Data Science!**
